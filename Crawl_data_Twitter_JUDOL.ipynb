{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MhThorq/CI-CD/blob/main/Crawl_data_Twitter_JUDOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawl Data Twitter > 2000 Tweets\n",
        "The crawling process was done using Tweet-Harvest. Written by Helmi Satria on  March 30th 2024.\n",
        "\n"
      ],
      "metadata": {
        "id": "xEWpayxURuUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Twitter Auth Token\n",
        "\n",
        "twitter_auth_token = '8d009e889d86955defe4b4af58c4c9dbb4ad19bf' # change this auth token"
      ],
      "metadata": {
        "id": "6S00x_f6-GeD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "4UIL1x21P9rQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4230c9e7-f242-4dcc-dda6-9aa8eaf765db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://deb.nodesource.com/node_20.x nodistro InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "curl: (23) Failed writing body\n",
            "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://deb.nodesource.com/node_20.x nodistro InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nodejs is already the newest version (20.19.6-1nodesource1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
            "v20.19.6\n"
          ]
        }
      ],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nama file hasil crawl (CSV)\n",
        "filename = 'tweets-judol-sentimen.csv'\n",
        "\n",
        "# Keyword Pencarian Khusus\n",
        "# Penjelasan Logic:\n",
        "# 1. (judol OR ...) -> Mencari kata kunci utama\n",
        "# 2. (rugi OR hancur ...) -> Memastikan tweet berisi kata-kata emosional/keluhan (agar bukan iklan bot)\n",
        "# 3. -filter:links -> Membuang tweet yang ada link-nya (karena 99% iklan judol pakai link)\n",
        "# 4. lang:id -> Hanya bahasa Indonesia\n",
        "search_keyword = '(judol OR \"judi online\" OR \"judi slot\") (rugi OR hancur OR bangkrut OR meresahkan OR stress OR \"bunuh diri\" OR tobat) -filter:links lang:id'\n",
        "\n",
        "# Jumlah data yang ingin diambil\n",
        "# Saran: Ambil minimal 500 - 1000 tweet agar model IndoBERT Anda punya cukup data belajar\n",
        "limit = 500\n",
        "\n",
        "# Perintah Crawling\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ],
      "metadata": {
        "id": "LYDR51dJlVlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc8b156-3b4a-4217-cd05-ac9c846182ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
            "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
            "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
            "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
            "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
            "\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mOpening twitter search page...\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "node:internal/process/promises:391\n",
            "    triggerUncaughtException(err, true /* fromPromise */);\n",
            "    ^\n",
            "\n",
            "browserType.launch: Target page, context or browser has been closed\n",
            "Browser logs:\n",
            "\n",
            "<launching> /root/.cache/ms-playwright/chromium_headless_shell-1200/chrome-headless-shell-linux64/chrome-headless-shell --disable-field-trial-config --disable-background-networking --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-back-forward-cache --disable-breakpad --disable-client-side-phishing-detection --disable-component-update --no-default-browser-check --disable-dev-shm-usage --disable-features=AcceptCHFrame,AvoidUnnecessaryBeforeUnloadCheckSync,DestroyProfileOnBrowserClose,DialMediaRouteProvider,GlobalMediaControls,HttpsUpgrades,LensOverlay,MediaRouter,PaintHolding,ThirdPartyStoragePartitioning,Translate,AutoDeElevate,RenderDocument,OptimizationHints --enable-features=CDPScreenshotNewSurface --allow-pre-commit-input --disable-hang-monitor --disable-ipc-flooding-protection --disable-popup-blocking --disable-prompt-on-repost --disable-renderer-backgrounding --force-color-profile=srgb --metrics-recording-only --no-first-run --password-store=basic --use-mock-keychain --no-service-autorun --export-tagged-pdf --disable-search-engine-choice-screen --unsafely-disable-devtools-self-xss-warnings --edge-skip-compat-layer-relaunch --enable-automation --disable-infobars --disable-search-engine-choice-screen --disable-sync --headless --hide-scrollbars --mute-audio --blink-settings=primaryHoverType=2,availableHoverTypes=2,primaryPointerType=4,availablePointerTypes=4 --no-sandbox --disable-blink-features=AutomationControlled --user-data-dir=/tmp/playwright_chromiumdev_profile-urrA0X --remote-debugging-pipe --no-startup-window\n",
            "<launched> pid=13808\n",
            "[pid=13808][err] /root/.cache/ms-playwright/chromium_headless_shell-1200/chrome-headless-shell-linux64/chrome-headless-shell: error while loading shared libraries: libatk-1.0.so.0: cannot open shared object file: No such file or directory\n",
            "Call log:\n",
            "\u001b[2m  - <launching> /root/.cache/ms-playwright/chromium_headless_shell-1200/chrome-headless-shell-linux64/chrome-headless-shell --disable-field-trial-config --disable-background-networking --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-back-forward-cache --disable-breakpad --disable-client-side-phishing-detection --disable-component-update --no-default-browser-check --disable-dev-shm-usage --disable-features=AcceptCHFrame,AvoidUnnecessaryBeforeUnloadCheckSync,DestroyProfileOnBrowserClose,DialMediaRouteProvider,GlobalMediaControls,HttpsUpgrades,LensOverlay,MediaRouter,PaintHolding,ThirdPartyStoragePartitioning,Translate,AutoDeElevate,RenderDocument,OptimizationHints --enable-features=CDPScreenshotNewSurface --allow-pre-commit-input --disable-hang-monitor --disable-ipc-flooding-protection --disable-popup-blocking --disable-prompt-on-repost --disable-renderer-backgrounding --force-color-profile=srgb --metrics-recording-only --no-first-run --password-store=basic --use-mock-keychain --no-service-autorun --export-tagged-pdf --disable-search-engine-choice-screen --unsafely-disable-devtools-self-xss-warnings --edge-skip-compat-layer-relaunch --enable-automation --disable-infobars --disable-search-engine-choice-screen --disable-sync --headless --hide-scrollbars --mute-audio --blink-settings=primaryHoverType=2,availableHoverTypes=2,primaryPointerType=4,availablePointerTypes=4 --no-sandbox --disable-blink-features=AutomationControlled --user-data-dir=/tmp/playwright_chromiumdev_profile-urrA0X --remote-debugging-pipe --no-startup-window\u001b[22m\n",
            "\u001b[2m  - <launched> pid=13808\u001b[22m\n",
            "\u001b[2m  - [pid=13808][err] /root/.cache/ms-playwright/chromium_headless_shell-1200/chrome-headless-shell-linux64/chrome-headless-shell: error while loading shared libraries: libatk-1.0.so.0: cannot open shared object file: No such file or directory\u001b[22m\n",
            "\u001b[2m  - [pid=13808] <gracefully close start>\u001b[22m\n",
            "\u001b[2m  - [pid=13808] <kill>\u001b[22m\n",
            "\u001b[2m  - [pid=13808] <will force kill>\u001b[22m\n",
            "\u001b[2m  - [pid=13808] exception while trying to kill process: Error: kill ESRCH\u001b[22m\n",
            "\u001b[2m  - [pid=13808] <process did exit: exitCode=127, signal=null>\u001b[22m\n",
            "\u001b[2m  - [pid=13808] starting temporary directories cleanup\u001b[22m\n",
            "\u001b[2m  - [pid=13808] finished temporary directories cleanup\u001b[22m\n",
            "\u001b[2m  - [pid=13808] <gracefully close end>\u001b[22m\n",
            "\n",
            "    at Proxy.launch (/root/.npm/_npx/d28e8c2154917da4/node_modules/\u001b[4mplaywright-extra\u001b[24m/dist/index.cjs.js:686:54)\n",
            "    at async crawl (/root/.npm/_npx/d28e8c2154917da4/node_modules/\u001b[4mtweet-harvest\u001b[24m/dist/bin.js:254:19)\n",
            "\n",
            "Node.js v20.19.6\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HvAG3hPvQDqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b659d0d-380d-4c49-c8c2-9f8e10294bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'tweets-judol-sentimen.csv' masih tidak ditemukan.\n",
            "Coba jalankan perintah '!ls' di sel baru untuk melihat nama file yang benar.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Nama file (pastikan SAMA PERSIS dengan output npx tweet-harvest tadi)\n",
        "filename = 'tweets-judol-sentimen.csv'\n",
        "\n",
        "# Hapus \"tweets-data/\" karena kemungkinan file ada di folder utama\n",
        "file_path = filename\n",
        "\n",
        "try:\n",
        "    # Coba baca file\n",
        "    df = pd.read_csv(file_path, delimiter=\",\") # Kadang delimiter bisa \";\" tergantung settingan lokal\n",
        "    print(\"Berhasil membaca file!\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{file_path}' masih tidak ditemukan.\")\n",
        "    print(\"Coba jalankan perintah '!ls' di sel baru untuk melihat nama file yang benar.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsGg8va4XoXC",
        "outputId": "259f6ee8-f53f-49d0-e1e5-8c627619ee8d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfDl54waHC4"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}